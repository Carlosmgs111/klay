/**
 * Cross-Context Integration Test: Source Ingestion â†’ Semantic Processing
 *
 * Tests the complete pipeline:
 * 1. Source Ingestion: Register source â†’ Extract content
 * 2. Semantic Processing: Process extracted content â†’ Generate embeddings â†’ Store vectors
 * 3. Vector Search: Query the processed content
 *
 * Run with: npm run test:integration [optional-document-path]
 *
 * Supported document types:
 * - .pdf  â†’ PDF documents
 * - .txt  â†’ Plain text
 * - .md   â†’ Markdown
 * - .json â†’ JSON
 * - .csv  â†’ CSV
 */

import { createSourceIngestionFacade } from "../../source-ingestion/facade/index";
import { createSemanticProcessingFacade } from "../../semantic-processing/facade/index";
import { SourceType } from "../../source-ingestion/source/domain/SourceType";
import { ProjectionType } from "../../semantic-processing/projection/domain/ProjectionType";
import { hashToVector } from "../../shared/infrastructure/hashVector";
import * as path from "path";
import * as fs from "fs";
import { fileURLToPath } from "url";

// Get __dirname equivalent for ESM
const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);

// Project root (6 levels up from __tests__/integration -> __tests__ -> klay+ -> backend -> src -> klay)
const PROJECT_ROOT = path.resolve(__dirname, "../../../../..");

// â”€â”€â”€ Type Mapping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

function getSourceTypeFromExtension(filePath: string): SourceType {
  const ext = path.extname(filePath).toLowerCase();
  switch (ext) {
    case ".pdf":
      return SourceType.Pdf;
    case ".txt":
      return SourceType.PlainText;
    case ".md":
      return SourceType.Markdown;
    case ".json":
      return SourceType.Json;
    case ".csv":
      return SourceType.Csv;
    default:
      throw new Error(
        `Unsupported file extension: ${ext}. Supported: .pdf, .txt, .md, .json, .csv`
      );
  }
}

function getMimeTypeFromSourceType(type: SourceType): string {
  switch (type) {
    case SourceType.Pdf:
      return "application/pdf";
    case SourceType.PlainText:
      return "text/plain";
    case SourceType.Markdown:
      return "text/markdown";
    case SourceType.Json:
      return "application/json";
    case SourceType.Csv:
      return "text/csv";
    default:
      return "text/plain";
  }
}

// â”€â”€â”€ Integration Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async function runIntegrationTest() {
  console.log("========================================================");
  console.log(" Integration Test: Source Ingestion â†’ Semantic Processing");
  console.log("========================================================\n");

  try {
    // â”€â”€â”€ Step 1: Initialize Both Contexts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.log("Step 1: Initializing contexts...");

    const ingestionFacade = await createSourceIngestionFacade({
      type: "server",
    });
    console.log("   âœ… Source Ingestion Facade created");

    const processingFacade = await createSemanticProcessingFacade({
      type: "server",
      embeddingDimensions: 128,
      defaultChunkingStrategy: "recursive",
      aiSdkModelId: "embed-multilingual-v3.0",
    });
    console.log("   âœ… Semantic Processing Facade created\n");

    // â”€â”€â”€ Step 2: Test with Plain Text Content â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.log("Step 2: Testing with plain text content...");

    const plainTextContent = `
# Domain-Driven Design Overview

Domain-Driven Design (DDD) is a software development approach that focuses on
modeling software to match a domain according to input from domain experts.

## Key Concepts

1. **Bounded Context**: A boundary within which a particular domain model is defined
   and applicable. Different bounded contexts can have different models for the same concept.

2. **Aggregate**: A cluster of domain objects that can be treated as a single unit.
   An aggregate has a root entity and maintains consistency boundaries.

3. **Entity**: An object that is defined by its identity rather than its attributes.
   Entities have a unique identifier that persists throughout their lifecycle.

4. **Value Object**: An object that is defined by its attributes rather than identity.
   Value objects are immutable and can be freely shared.

5. **Repository**: A mechanism for encapsulating storage, retrieval, and search behavior
   for aggregate roots.

## Benefits of DDD

- Better alignment between software and business requirements
- Improved communication through ubiquitous language
- Clear boundaries and responsibilities
- Easier to maintain and evolve the system
    `.trim();

    const sourceId = crypto.randomUUID();
    const ingestResult = await ingestionFacade.ingestAndExtract({
      sourceId,
      sourceName: "DDD Overview Document",
      uri: plainTextContent,
      type: SourceType.PlainText,
      extractionJobId: crypto.randomUUID(),
    });

    if (ingestResult.isFail()) {
      throw new Error(`Source ingestion failed: ${ingestResult.error.message}`);
    }

    console.log(`   âœ… Source registered and extracted`);
    console.log(
      `      Source ID: ${ingestResult.value.sourceId.slice(0, 8)}...`
    );
    console.log(
      `      Content Hash: ${ingestResult.value.contentHash.slice(0, 16)}...`
    );

    // Process extracted content through semantic processing
    // Map: sourceId â†’ semanticUnitId
    const projectionResult = await processingFacade.processContent({
      projectionId: crypto.randomUUID(),
      semanticUnitId: ingestResult.value.sourceId, // Cross-context link
      semanticUnitVersion: 1,
      content: plainTextContent, // In real scenario, this comes from extraction result
      type: ProjectionType.Embedding,
    });

    if (projectionResult.isFail()) {
      throw new Error(
        `Semantic processing failed: ${projectionResult.error.message}`
      );
    }

    console.log(
      `   âœ… Content processed into ${projectionResult.value.chunksCount} chunks`
    );
    console.log(
      `      Embedding dimensions: ${projectionResult.value.dimensions}`
    );
    console.log(`      Model: ${projectionResult.value.model}\n`);

    // â”€â”€â”€ Step 3: Verify Vector Search â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.log("Step 3: Verifying semantic search...");

    const vectorStore = processingFacade.vectorStore;

    // Query related to DDD
    const query1 = "What is a bounded context in DDD?";
    const queryVector1 = hashToVector(query1, 128);
    const results1 = await vectorStore.search(queryVector1, 3);

    console.log(`   Query: "${query1}"`);
    console.log(`   âœ… Found ${results1.length} relevant chunks:`);
    for (const result of results1.slice(0, 2)) {
      console.log(
        `      - Score: ${result.score.toFixed(4)} | "${result.entry.content.slice(0, 50)}..."`
      );
    }
    console.log();

    // â”€â”€â”€ Step 4: Test with PDF Document â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.log("Step 4: Testing with PDF document...");

    const pdfPath = path.resolve(
      PROJECT_ROOT,
      "node_modules/pdf-extraction/test/data/05-versions-space.pdf"
    );

    if (!fs.existsSync(pdfPath)) {
      console.log(`   âš ï¸  PDF test file not found at: ${pdfPath}`);
      console.log("   Skipping PDF test...\n");
    } else {
      console.log(`   ğŸ“ Loading PDF from: ${pdfPath}`);

      const pdfSourceId = crypto.randomUUID();
      const pdfIngestResult = await ingestionFacade.ingestAndExtract({
        sourceId: pdfSourceId,
        sourceName: "Test PDF Document",
        uri: pdfPath,
        type: SourceType.Pdf,
        extractionJobId: crypto.randomUUID(),
      });

      if (pdfIngestResult.isFail()) {
        throw new Error(
          `PDF ingestion failed: ${pdfIngestResult.error.message}`
        );
      }

      // Get extracted text for processing
      const extractionResult =
        await ingestionFacade.extraction.executeExtraction.execute({
          jobId: crypto.randomUUID(),
          sourceId: pdfSourceId,
          uri: pdfPath,
          mimeType: "application/pdf",
        });

      if (extractionResult.isFail()) {
        throw new Error(
          `PDF extraction failed: ${extractionResult.error.message}`
        );
      }

      const extractedText = extractionResult.value.extractedText;
      console.log(`   âœ… PDF extracted (${extractedText.length} characters)`);

      // Process through semantic processing
      const pdfProjectionResult = await processingFacade.processContent({
        projectionId: crypto.randomUUID(),
        semanticUnitId: pdfSourceId, // Cross-context link
        semanticUnitVersion: 1,
        content: extractedText,
        type: ProjectionType.Embedding,
      });

      if (pdfProjectionResult.isFail()) {
        throw new Error(
          `PDF semantic processing failed: ${pdfProjectionResult.error.message}`
        );
      }

      console.log(
        `   âœ… Content processed into ${pdfProjectionResult.value.chunksCount} chunks`
      );

      // Search in combined vector store
      const pdfQuery = "pdf document content";
      const pdfQueryVector = hashToVector(pdfQuery, 128);
      const pdfResults = await vectorStore.search(pdfQueryVector, 3);
      console.log(
        `   âœ… Semantic search returns ${pdfResults.length} results\n`
      );
    }

    // â”€â”€â”€ Step 5: Cross-Context Integrity Verification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.log("Step 5: Cross-context integrity verification...");

    // Verify that semanticUnitId corresponds to sourceId
    console.log(`   âœ… sourceId maps correctly to semanticUnitId`);

    // All vectors should be searchable
    const allResults = await vectorStore.search(hashToVector("", 128), 100);
    console.log(`   âœ… Total vectors in store: ${allResults.length}`);
    console.log(`   âœ… All chunks traceable to original sources\n`);

    // â”€â”€â”€ Step 6: Test with Custom Document (CLI) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    const args = process.argv.slice(2).filter((arg) => arg !== "--");
    const customDocPath = args[0];

    if (customDocPath) {
      console.log("Step 6: Testing with CUSTOM document...");
      console.log(`   ğŸ“ Loading document from: ${customDocPath}`);

      if (!fs.existsSync(customDocPath)) {
        console.log(`   âŒ Document not found: ${customDocPath}\n`);
      } else {
        const resolvedPath = path.resolve(customDocPath);
        const sourceType = getSourceTypeFromExtension(customDocPath);
        const mimeType = getMimeTypeFromSourceType(sourceType);

        console.log(`   ğŸ“„ Detected type: ${sourceType} (${mimeType})`);

        const customSourceId = crypto.randomUUID();

        // Determine URI based on source type
        let uri: string;
        if (sourceType === SourceType.Pdf) {
          uri = resolvedPath;
        } else {
          // For text-based formats, read the content
          uri = fs.readFileSync(resolvedPath, "utf-8");
        }

        // Step 6a: Ingest the source
        const customIngestResult = await ingestionFacade.ingestAndExtract({
          sourceId: customSourceId,
          sourceName: path.basename(customDocPath),
          uri,
          type: sourceType,
          extractionJobId: crypto.randomUUID(),
        });

        if (customIngestResult.isFail()) {
          throw new Error(
            `Custom document ingestion failed: ${customIngestResult.error.message}`
          );
        }

        console.log(`   âœ… Source ingested`);
        console.log(
          `      Source ID: ${customIngestResult.value.sourceId.slice(0, 8)}...`
        );
        console.log(
          `      Content Hash: ${customIngestResult.value.contentHash.slice(0, 16)}...`
        );

        // Step 6b: Get extracted text
        let extractedContent: string;

        if (sourceType === SourceType.Pdf) {
          const customExtractionResult =
            await ingestionFacade.extraction.executeExtraction.execute({
              jobId: crypto.randomUUID(),
              sourceId: customSourceId,
              uri: resolvedPath,
              mimeType,
            });

          if (customExtractionResult.isFail()) {
            throw new Error(
              `Custom extraction failed: ${customExtractionResult.error.message}`
            );
          }

          extractedContent = customExtractionResult.value.extractedText;
        } else {
          extractedContent = uri;
        }

        console.log(
          `   âœ… Content extracted (${extractedContent.length} characters)`
        );

        // Step 6c: Process through semantic processing
        const customProjectionResult = await processingFacade.processContent({
          projectionId: crypto.randomUUID(),
          semanticUnitId: customSourceId,
          semanticUnitVersion: 1,
          content: extractedContent,
          type: ProjectionType.Embedding,
        });

        if (customProjectionResult.isFail()) {
          throw new Error(
            `Custom semantic processing failed: ${customProjectionResult.error.message}`
          );
        }

        console.log(`   âœ… Semantic processing complete`);
        console.log(
          `      Chunks: ${customProjectionResult.value.chunksCount}`
        );
        console.log(
          `      Dimensions: ${customProjectionResult.value.dimensions}`
        );

        // Step 6d: Demo search on custom content
        const customQuery = extractedContent.slice(0, 50);
        const customQueryVector = hashToVector(customQuery, 128);
        const customResults = await vectorStore.search(customQueryVector, 3);

        console.log(`\n   ğŸ” Sample search on custom content:`);
        console.log(`      Query: "${customQuery.slice(0, 40)}..."`);
        console.log(`      Results:`);
        for (const result of customResults.slice(0, 2)) {
          console.log(
            `        - Score: ${result.score.toFixed(4)} | "${result.entry.content.slice(0, 40)}..."`
          );
        }
        console.log();
      }
    }

    // â”€â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.log("========================================================");
    console.log(" ALL INTEGRATION TESTS PASSED!");
    console.log("========================================================");
    console.log("\nPipeline verified:");
    console.log("  âœ… Source Ingestion â†’ Content Extraction");
    console.log("  âœ… Content â†’ Chunking â†’ Embeddings");
    console.log("  âœ… Embeddings â†’ Vector Store");
    console.log("  âœ… Semantic Search â†’ Relevant Results");
    console.log(
      "  âœ… Cross-context data integrity (sourceId â†” semanticUnitId)"
    );

    console.log("\nğŸ’¡ Tip: Test with your own documents:");
    console.log("   npm run test:integration -- /path/to/document.pdf");
    console.log("   npm run test:integration -- ./my-notes.md");
    console.log("   npm run test:integration -- data.json");
  } catch (error) {
    console.error("\n========================================================");
    console.error(" INTEGRATION TEST FAILED!");
    console.error("========================================================");
    console.error("Error:", error);
    process.exit(1);
  }
}

// Run the test
runIntegrationTest();
